{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1차원 tensor는 vector\n",
    "* 2차원 tensor는 matrix\n",
    "* 2차원보다 큰 차원을 가진 것들은 그냥 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # 연구환경에서 연구 결과의 재현 가능성에 대한 확신을 얻고 싶을 때 모델의 학습 가중치와 같은 무작위 값을 가진 tensor로 초기화하는 것\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor의 shape\n",
    "\n",
    "두개 이상의 tensor에 대한 연산을 수행할 때, tensor들은 같은 shape를 필요로함.\n",
    "차원의 개수가 같아야 하고, 각 차원마다 원소 수가 같아야한다.\n",
    "torch.*_like() 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서생성, Pytorch Collection 형식의 데이터를 직접적으로 명시\n",
    "\n",
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)\n",
    "\n",
    "# torch.tensor()로 이미 튜플이나 리스트 형태로 이루어진 데이터를 가지고 있는 경우\n",
    "# 중첩된 형태의 collection 자료형은 다차원 텐서로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_constants.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_integers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_integers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Broadcasting\n",
    "\n",
    "텐서는 같은 shape 끼리만 연산이 가능하다는 규칙의 예외가 tensor broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6146, 0.5999, 0.5013, 0.9397],\n",
      "        [0.8656, 0.5207, 0.6865, 0.3614]])\n",
      "tensor([[1.2291, 1.1998, 1.0026, 1.8793],\n",
      "        [1.7312, 1.0413, 1.3730, 0.7228]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]]])\n",
      "tensor([[[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]]])\n",
      "tensor([[[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 세번째와 두번째 차원이 a랑 동일하고, 첫번째 차원은 존재하지 않습니다.\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 세번째 차원 = 1이고, 두번째 차원은 a랑 동일합니다.\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 세번째 차원이 a랑 동일하고, 두번째 차원 = 1입니다.\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytorch의 broadcasting : 다른 크기의 텐서간에 연산을 수행할 수 있게 해주는 기능\n",
    "broadcasting을 통해 크기가 다른 텐서간의 연산을 자동으로 확장하여 일치시킴.\n",
    "\n",
    "1. 모양이 다를 경우 : 각 차언에서 두 텐서 모양이 다르면 더 작은 모양의 텐서가 더 큰 모양의 텐서와 동일한 모양을 가지도록 확장됨.\n",
    "\n",
    "2. 뒤쪽 차원부터 비교\n",
    "\n",
    "3. 크기가 1인 차원 : 차원의 크기가 1인 경우 해당 차원은 다른 텐서의 해당 차원과 일치하도록 확장됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.9238, 0.5724, 0.0791, 0.2629],\n",
      "        [0.1986, 0.4439, 0.6434, 0.4776]])\n",
      "tensor([[-0., -0., 1., -0.],\n",
      "        [-0., 1., 1., -0.]])\n",
      "tensor([[-1., -1.,  0., -1.],\n",
      "        [-1.,  0.,  0., -1.]])\n",
      "tensor([[-0.5000, -0.5000,  0.0791, -0.2629],\n",
      "        [-0.1986,  0.4439,  0.5000, -0.4776]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.7375, 0.8328],\n",
      "        [0.8444, 0.2941]])\n",
      "tensor([[2.2125, 2.4985],\n",
      "        [2.5332, 0.8822]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.7889, -0.6145],\n",
      "        [-0.6145,  0.7889]]),\n",
      "S=tensor([4.1498, 1.0548]),\n",
      "V=tensor([[-0.7957,  0.6056],\n",
      "        [-0.6056, -0.7957]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HeawonJeong\\AppData\\Local\\Temp\\ipykernel_22084\\3529175983.py:46: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Cross.cpp:67.)\n",
      "  print(torch.cross(v2, v1)) # z축 단위 벡터의 음수값 (v1 x v2 == -v2 x v1)\n"
     ]
    }
   ],
   "source": [
    "# 공용 함수\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# 삼각 함수와 그 역함수들\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# 비트 연산\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# 비교 연산:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # 많은 비교 연산자들은 broadcasting을 지원합니다!\n",
    "print(torch.eq(d, e)) # bool 자료형을 가진 tensor를 반환합니다.\n",
    "\n",
    "# 차원 감소 연산:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # 단일 원소 tensor를 반환합니다.\n",
    "print(torch.max(d).item()) # 반환한 tensor로부터 값을 추출합니다.\n",
    "print(torch.mean(d))       # 평균\n",
    "print(torch.std(d))        # 표준 편차\n",
    "print(torch.prod(d))       # 모든 숫자의 곱\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # 중복되지 않은 값들을 걸러냅니다.\n",
    "\n",
    "# 벡터와 선형 대수 연산\n",
    "v1 = torch.tensor([1., 0., 0.])         # x축 단위 벡터\n",
    "v2 = torch.tensor([0., 1., 0.])         # y축 단위 벡터\n",
    "m1 = torch.rand(2, 2)                   # 무작위 행렬\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # 단위 행렬에 3을 곱한 결과\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # z축 단위 벡터의 음수값 (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # m1 행렬을 3번 곱한 결과\n",
    "print(torch.svd(m3))       # 특이값 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # 이 연산은 메모리에 새로운 tensor를 생성합니다.\n",
    "print(a)              # a는 변하지 않습니다.\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # 밑줄에 주목하세요.\n",
    "print(b)              # b가 변합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.3788, 0.4567],\n",
      "        [0.0649, 0.6677]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.3788, 1.4567],\n",
      "        [1.0649, 1.6677]])\n",
      "tensor([[1.3788, 1.4567],\n",
      "        [1.0649, 1.6677]])\n",
      "tensor([[0.3788, 0.4567],\n",
      "        [0.0649, 0.6677]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.1435, 0.2086],\n",
      "        [0.0042, 0.4459]])\n",
      "tensor([[0.1435, 0.2086],\n",
      "        [0.0042, 0.4459]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.7826, 0.1332],\n",
      "        [0.0023, 0.4945]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.7826, 1.1332],\n",
      "        [1.0023, 1.4945]])\n",
      "tensor([[1.7826, 1.1332],\n",
      "        [1.0023, 1.4945]])\n",
      "tensor([[0.7826, 0.1332],\n",
      "        [0.0023, 0.4945]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[6.1248e-01, 1.7732e-02],\n",
      "        [5.1415e-06, 2.4454e-01]])\n",
      "tensor([[6.1248e-01, 1.7732e-02],\n",
      "        [5.1415e-06, 2.4454e-01]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1.0062, 0.1893],\n",
      "        [0.7523, 0.2162]])\n",
      "tensor([[0.0905, 0.4485],\n",
      "        [0.8740, 0.2526]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # c의 값이 변경되었습니다.\n",
    "\n",
    "\n",
    "# assert 문은 파이썬의 디버깅 목적으로 사용된ㄴ 도구\n",
    "# assert condition, message : condition에 부합하지 않으면 AssertionError 발생\n",
    "assert c is d           # c와 d가 서로 단순히 같은 값을 가지는지가 아니라 같은 객체인지 테스트합니다.\n",
    "assert id(c) == old_id  # 새로운 c는 이전 객체와 확실히 같은 객체입니다.\n",
    "\n",
    "torch.rand(2, 2, out=c) # 다시 한번 생성해봅시다!\n",
    "print(c)                # c의 값이 다시 바뀌었습니다.\n",
    "assert id(c) == old_id  # 하지만 여전히 같은 객체네요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor를 복사하기\n",
    "\n",
    "변수에 텐서를 할당하는 것은변수가 tensor의 라벨이 되고 값을 복사하지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # a의 값을 바꾸면...\n",
    "print(b)       # ...b의 값이 바뀝니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# clone 메소드\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # 메모리 상의 다른 객체입니다...\n",
    "print(torch.eq(a, b))  # ...하지만 여전히 같은 값을 가지고 있네요!\n",
    "\n",
    "a[0][1] = 561          # a가 변경되었습니다...\n",
    "print(b)               # ...하지만 여전히 b는 이전 값을 가지고 있네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clone() 메소드를 사용할 때 소스텐서가 autograd를 가지면 clone이 가능하다. \n",
    "만약 모델이 그 모델의 forward() 메소드에 여러갈래의 계산 경로가 있고, 원본 텐서와 그것의 복제본 모두가 모델의 출력에 기여한다면, 두 텐서에 대한 autograd를 설정하는 모델 학습을 활성화한다.\n",
    "그리고 원본 텐서나 복제본 모두 변화도를 추저갈 필요가 없을 때 소스텐서의 autograd가 꺼져있다면 clone을 사용할 수 있다. \n",
    "\n",
    "세번째 경우, 변화도가 모든 것을 위해 켜져 있지만 일부 지표를 생성하기 위해서 스트림 중간에서 일부 값을 생성하고 싶어하는 모델의 forward() 함수에서 계산을 수행한다고 상상해보았을 때, source tensor에 .detach()메소드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6923, 0.7545],\n",
      "        [0.7746, 0.2330]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # autograd와 계산 히스토리 추적기능을 켠다.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6923, 0.7545],\n",
      "        [0.7746, 0.2330]], grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a.clone() # a를 복제하고 b라고 라벨을 붙임. b를 출력할 때 계산 히스토리가 추적되고 있음을 확인.\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6923, 0.7545],\n",
      "        [0.7746, 0.2330]])\n",
      "tensor([[0.6923, 0.7545],\n",
      "        [0.7746, 0.2330]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "c = a.detach().clone()\n",
    "print(c) # c에 계산 히스토리가 없음, requires_grad=True 옵션이 없음.\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 환경으로 이동하기\n",
    "\n",
    "Pytorch의 주된 장점은 CUDA가 호환되는 Nvidia GPU에서의 강력한 성능 가속화이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a Gpu!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have a Gpu!')\n",
    "else:\n",
    "    print('CPU only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 GPU가 확인할 수 있는 어떤 공간에 저장해야 한다. CPU는 컴퓨터의 RAM에서 데이터를 이용해서 계산을 수행한다.\n",
    "\n",
    "GPU는 전용 메모리가 연결되어 있는데, GPU에서 계산을 수행하고 싶을 때마다 계산에 필요한 모든 데이터를 GPU장치가 접근 가능한 메모리로 이동해야 한다.\n",
    "\n",
    "'GPU가 접근 가능한 메모리로 데이터를 이동한다' == '데이터를 GPU로 옮긴다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3344, 0.2640],\n",
      "        [0.2119, 0.0582]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "\n",
    "else:\n",
    "    print('sorry cpu only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 새로운 텐서는 CPU에 생성되므로, 텐서를 GPU에 생성하고 싶을 때 device 인자를 반드시 명시해주어야한다. \n",
    "새로운 텐서를 출력할 때, 파이토치는 어느 장치에 객체가 있는지 알려준다.\n",
    "\n",
    "* torch.cuda.device_count()를 사용해서 GPU 개수를 조회함\n",
    "* device='cuda:0', device='cuda:1'\n",
    "\n",
    "장치 이름을 문자열 상수로 지정하는 것은 유지보수에 취약함.\n",
    "문자열 대신에 텐서를 저장할 장치 핸들러를 생성하는 것으로 유지 보수가 쉬운 코드를 작성할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n",
      "tensor([[0.0024, 0.6778],\n",
      "        [0.2441, 0.6812]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device=torch.device('cuda')\n",
    "else:\n",
    "    my_device=torch.device('cpu')\n",
    "print('Device : {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 장치에 텐서가 있을 때, to() 메소드를 사용해서 다른 장치로 이동할 수 있음.\n",
    "# cpu에 텐서를 생성하고, 이전 셀에서 얻은 장치 핸들러로 텐서를 이동시킴\n",
    "\n",
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rand() received an invalid combination of arguments - got (int, int, deivce=str), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# !! 2개 이상의 텐서를 포함한 계산을 하기 위해서는 모든 텐서가 같은 장치에 있어야함.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# runtime error\u001b[39;00m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeivce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: rand() received an invalid combination of arguments - got (int, int, deivce=str), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# !! 2개 이상의 텐서를 포함한 계산을 하기 위해서는 모든 텐서가 같은 장치에 있어야함.\n",
    "# runtime error\n",
    "\n",
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2, deivce='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor의 shape 다루기\n",
    "\n",
    "### 차원의 개수 변경하기\n",
    "모델의 입력에 단일 인스턴스를 전달할 때, 차원의개수를 변경할 필요가 있다. 파이토치에서 일반적으로 입력에 배치가 들어온다.\n",
    "\n",
    "ex. 3개의 색깔 채널을 가진 226x226 image\n",
    "\n",
    "이미지를 불러오고 텐서로 변환하면 (3, 226, 226) shape을 가진 텐서가 됨. --> (N, 3, 226, 226)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "# unsqueeze() 메소드는 크기가 1인 차원을 추가\n",
    "# unsqueeze(0)은 새로운 0번째 차원을 추가\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.7112]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "# 차원을 하나 확장해도 텐서에 있는 원소의 개수는 변하지 않는다.\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex. 각 입력값에 대한 모델의 출력값이 20개의 원소를 가진 vector\n",
    "* N : 입력 배치에 있는 인스턴스의 개수\n",
    "* 출력  (N, 20)\n",
    "\n",
    "배치 shape이 아닌 연산 결과를 얻고싶다면??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.8203, 0.3941, 0.7000, 0.6464, 0.6506, 0.7114, 0.1617, 0.5450, 0.5948,\n",
      "         0.4542, 0.0465, 0.7545, 0.7612, 0.3475, 0.2052, 0.6022, 0.1849, 0.2320,\n",
      "         0.0269, 0.2416]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n",
      "tensor([0.8203, 0.3941, 0.7000, 0.6464, 0.6506, 0.7114, 0.1617, 0.5450, 0.5948,\n",
      "        0.4542, 0.0465, 0.7545, 0.7612, 0.3475, 0.2052, 0.6022, 0.1849, 0.2320,\n",
      "        0.0269, 0.2416])\n"
     ]
    }
   ],
   "source": [
    "b= a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(2, 2)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "d= c.squeeze(0) # 차원의 값이 1이 아니므로 squeeze가 작동하지 않음.\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오직 차원의 값이 1인 경우에만 squeeze()를 사용할 수 있다. 이 경우가 아니면 텐서의 원소 개수가 바뀌기 때문이다.\n",
    "\n",
    "unsqueeze는 broadcasting을 쉽게 하는 경우에도 사용된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0164, 0.0164],\n",
      "         [0.5220, 0.5220],\n",
      "         [0.5693, 0.5693]],\n",
      "\n",
      "        [[0.0164, 0.0164],\n",
      "         [0.5220, 0.5220],\n",
      "         [0.5693, 0.5693]],\n",
      "\n",
      "        [[0.0164, 0.0164],\n",
      "         [0.5220, 0.5220],\n",
      "         [0.5693, 0.5693]],\n",
      "\n",
      "        [[0.0164, 0.0164],\n",
      "         [0.5220, 0.5220],\n",
      "         [0.5693, 0.5693]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "c = a * torch.rand(  3, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.6086, 0.6086],\n",
      "         [0.5455, 0.5455],\n",
      "         [0.0417, 0.0417]],\n",
      "\n",
      "        [[0.6086, 0.6086],\n",
      "         [0.5455, 0.5455],\n",
      "         [0.0417, 0.0417]],\n",
      "\n",
      "        [[0.6086, 0.6086],\n",
      "         [0.5455, 0.5455],\n",
      "         [0.0417, 0.0417]],\n",
      "\n",
      "        [[0.6086, 0.6086],\n",
      "         [0.5455, 0.5455],\n",
      "         [0.0417, 0.0417]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # a * b를 시도하는 것은 runtime error가 발생합니다.\n",
    "c = b.unsqueeze(1)       # 끝에 새로운 차원을 추가해서 2차원 tensor로 바꿉니다.\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting이 다시 작동합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원소의 개수와 원소의 값은 여전히 유지하면서 텐서의 shape을 한번에 바꾸고 싶을 때가 있다. 모델의 conv layer와 linear layer 사이 인터페이스.\n",
    "\n",
    "conv layer는 특성의 수 x 너비 x 높이 shape의 텐서를 출ㄹ격을 생성\n",
    "\n",
    "linear layer는 입력값으로 1차원을 받음.\n",
    "\n",
    "reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "input1d = output3d.reshape(6*20*20)\n",
    "print(input1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "print(torch.reshape(output3d, (6*20*20,)).shape) # (6*20*20,)는 텐서 쉐입을 나타낼 때 튜플을 기대하므로 이 인자가 진짜 1개 원소를 가진 튜플이라고 하기 위해 컴마 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서를 바라보는 관점. 메모리의 같은 지역을 바라보는 서로 다른 관점을 가진 텐서 객체\n",
    "\n",
    "소스텐서에 어떠한 변화가 있으면 해당 텐서를 바라보고 있는 다른 객체 또한 값이 변한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy로 변환\n",
    "\n",
    "NumPy의 ndarrays에 저장되어있는 데이터를 사용하는 코드를 가지고 있다면, 같은 데이터를 파이토치의 GPU가속을 사용할수 있고 파이토치 텐서로 표현할 수 있다.\n",
    "ndarray와 파이토치끼리 바꿀 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2926, 0.6360, 0.9112],\n",
      "        [0.9312, 0.3967, 0.2918]])\n",
      "[[0.2925921  0.63602173 0.9111862 ]\n",
      " [0.93117917 0.39667493 0.2918461 ]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.2925921   0.63602173  0.9111862 ]\n",
      " [ 0.93117917 17.          0.2918461 ]]\n"
     ]
    }
   ],
   "source": [
    "# 변환된 객체들을 해당 객체의 소스객체가 위치한 메모리의 같은 공간을 사용한다.\n",
    "# 즉 한 객체가 변하면 다른 것에 영향을 준다.\n",
    "\n",
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
